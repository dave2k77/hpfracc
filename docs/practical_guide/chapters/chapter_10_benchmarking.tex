\section{Benchmarking Overview}

Choosing the right backend can lead to 100x speedups. This chapter analyzes the performance trade-offs between NumPy, Numba, JAX, and Torch backends.

\section{Comparison Results}

\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
Frontend & Backend & Size ($10^5$) & Time (ms) \\ \midrule
CPU      & NumPy   & Slow          & 450       \\
CPU      & Numba   & Fast          & 12        \\
GPU      & JAX     & Fastest       & 0.8       \\
GPU      & Torch   & Fast          & 1.2       \\ \bottomrule
\end{tabular}
\caption{Benchmark results for fractional derivative computation.}
\end{table}

\section{Running Benchmarks}

\begin{lstlisting}[language=Python]
from hpfracc.ml.backends import BackendManager, BackendType
import time

def benchmark(backend_type):
    with BackendManager(backend_type):
        start = time.time()
        # ... operation ...
        return time.time() - start
\end{lstlisting}

\section{Conclusion}
For production workloads, use JAX or Torch. For small-scale research scripts, Numba provides excellent performance without the overhead of heavy ML frameworks.
